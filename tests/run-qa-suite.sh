#!/bin/bash\n\n# FormaOS Quality Assurance Testing Suite\n# Comprehensive testing script for Phase 5 implementation\n\nset -euo pipefail\n\n# Colors for output\nRED='\\033[0;31m'\nGREEN='\\033[0;32m'\nYELLOW='\\033[1;33m'\nBLUE='\\033[0;34m'\nNC='\\033[0m' # No Color\n\n# Configuration\nLOGS_DIR=\"tests/logs\"\nRESULTS_DIR=\"tests/results\"\nTIMESTAMP=$(date +\"%Y%m%d_%H%M%S\")\nTEST_REPORT=\"qa_test_report_${TIMESTAMP}.json\"\n\n# Ensure directories exist\nmkdir -p \"$LOGS_DIR\" \"$RESULTS_DIR\"\n\n# Function to print colored output\nprint_status() {\n    local status=\"$1\"\n    local message=\"$2\"\n    case $status in\n        \"INFO\")  echo -e \"${BLUE}[INFO]${NC} $message\" ;;\n        \"SUCCESS\") echo -e \"${GREEN}[SUCCESS]${NC} $message\" ;;\n        \"WARNING\") echo -e \"${YELLOW}[WARNING]${NC} $message\" ;;\n        \"ERROR\") echo -e \"${RED}[ERROR]${NC} $message\" ;;\n    esac\n}\n\n# Function to log test results\nlog_result() {\n    local test_name=\"$1\"\n    local status=\"$2\"\n    local duration=\"$3\"\n    local details=\"$4\"\n    \n    echo \"{\n  \\\"test\\\": \\\"$test_name\\\",\n  \\\"status\\\": \\\"$status\\\",\n  \\\"duration\\\": $duration,\n  \\\"timestamp\\\": \\\"$(date -u +%Y-%m-%dT%H:%M:%SZ)\\\",\n  \\\"details\\\": \\\"$details\\\"\n},\" >> \"$RESULTS_DIR/$TEST_REPORT\"\n}\n\n# Start test report\necho \"[\" > \"$RESULTS_DIR/$TEST_REPORT\"\n\nprint_status \"INFO\" \"ðŸ§ª Starting FormaOS Quality Assurance Testing Suite\"\nprint_status \"INFO\" \"ðŸ“Š Test Report: $TEST_REPORT\"\nprint_status \"INFO\" \"â° Started at: $(date)\"\n\n# Phase 1: Environment Setup\nprint_status \"INFO\" \"ðŸ”§ Phase 1: Environment Setup\"\nstart_time=$(date +%s)\n\n# Check Node.js and npm\nif command -v node > /dev/null && command -v npm > /dev/null; then\n    NODE_VERSION=$(node --version)\n    NPM_VERSION=$(npm --version)\n    print_status \"SUCCESS\" \"Node.js $NODE_VERSION and npm $NPM_VERSION detected\"\nelse\n    print_status \"ERROR\" \"Node.js or npm not found\"\n    exit 1\nfi\n\n# Check if development server is running\nif curl -s http://localhost:3000 > /dev/null 2>&1; then\n    print_status \"SUCCESS\" \"Development server is running on localhost:3000\"\n    SERVER_RUNNING=true\nelse\n    print_status \"WARNING\" \"Development server not detected, starting...\"\n    npm run dev > \"$LOGS_DIR/dev_server.log\" 2>&1 &\n    DEV_SERVER_PID=$!\n    sleep 10\n    if curl -s http://localhost:3000 > /dev/null 2>&1; then\n        print_status \"SUCCESS\" \"Development server started successfully\"\n        SERVER_RUNNING=true\n    else\n        print_status \"ERROR\" \"Failed to start development server\"\n        SERVER_RUNNING=false\n    fi\nfi\n\nend_time=$(date +%s)\nlog_result \"Environment Setup\" \"$([ $SERVER_RUNNING = true ] && echo success || echo failed)\" $((end_time - start_time)) \"Node.js $NODE_VERSION, npm $NPM_VERSION\"\n\n# Phase 2: Static Analysis\nprint_status \"INFO\" \"ðŸ” Phase 2: Static Analysis & Linting\"\nstart_time=$(date +%s)\n\n# TypeScript compilation check\nprint_status \"INFO\" \"Checking TypeScript compilation...\"\nif npm run type-check > \"$LOGS_DIR/typescript.log\" 2>&1; then\n    print_status \"SUCCESS\" \"TypeScript compilation passed\"\n    TS_STATUS=\"success\"\nelse\n    print_status \"ERROR\" \"TypeScript compilation failed\"\n    TS_STATUS=\"failed\"\nfi\n\n# ESLint check\nprint_status \"INFO\" \"Running ESLint analysis...\"\nif npm run lint > \"$LOGS_DIR/eslint.log\" 2>&1; then\n    print_status \"SUCCESS\" \"ESLint analysis passed\"\n    ESLINT_STATUS=\"success\"\nelse\n    print_status \"WARNING\" \"ESLint found issues (check logs)\"\n    ESLINT_STATUS=\"warning\"\nfi\n\nend_time=$(date +%s)\nlog_result \"Static Analysis\" \"$([ $TS_STATUS = success ] && [ $ESLINT_STATUS != failed ] && echo success || echo failed)\" $((end_time - start_time)) \"TypeScript: $TS_STATUS, ESLint: $ESLINT_STATUS\"\n\n# Phase 3: Unit Testing\nprint_status \"INFO\" \"ðŸ§ª Phase 3: Unit Testing\"\nstart_time=$(date +%s)\n\nif npm run test:coverage > \"$LOGS_DIR/unit_tests.log\" 2>&1; then\n    print_status \"SUCCESS\" \"Unit tests passed\"\n    # Extract coverage information\n    if grep -q \"All files\" \"$LOGS_DIR/unit_tests.log\"; then\n        COVERAGE=$(grep \"All files\" \"$LOGS_DIR/unit_tests.log\" | awk '{print $10}')\n        print_status \"INFO\" \"Code coverage: $COVERAGE\"\n    fi\n    UNIT_STATUS=\"success\"\nelse\n    print_status \"ERROR\" \"Unit tests failed\"\n    UNIT_STATUS=\"failed\"\nfi\n\nend_time=$(date +%s)\nlog_result \"Unit Testing\" \"$UNIT_STATUS\" $((end_time - start_time)) \"Coverage: ${COVERAGE:-unknown}\"\n\n# Phase 4: Visual Regression Testing\nif [ \"$SERVER_RUNNING\" = true ]; then\n    print_status \"INFO\" \"ðŸ‘ï¸ Phase 4: Visual Regression Testing\"\n    start_time=$(date +%s)\n    \n    if npm run test:visual > \"$LOGS_DIR/visual_tests.log\" 2>&1; then\n        print_status \"SUCCESS\" \"Visual regression tests passed\"\n        VISUAL_STATUS=\"success\"\n    else\n        print_status \"WARNING\" \"Visual regression tests failed - check for UI changes\"\n        VISUAL_STATUS=\"failed\"\n        \n        # Try to approve new references if this is expected\n        print_status \"INFO\" \"Generating new visual references...\"\n        npm run test:visual:reference > \"$LOGS_DIR/visual_reference.log\" 2>&1 || true\n    fi\n    \n    end_time=$(date +%s)\n    log_result \"Visual Regression\" \"$VISUAL_STATUS\" $((end_time - start_time)) \"BackstopJS visual comparison\"\nelse\n    print_status \"WARNING\" \"Skipping visual tests - server not running\"\n    VISUAL_STATUS=\"skipped\"\nfi\n\n# Phase 5: Load Testing\nif [ \"$SERVER_RUNNING\" = true ]; then\n    print_status \"INFO\" \"âš¡ Phase 5: Load Testing\"\n    start_time=$(date +%s)\n    \n    # Run quick load test\n    if artillery run tests/load/quick-test.yml --output \"$RESULTS_DIR/load_test_$TIMESTAMP.json\" > \"$LOGS_DIR/load_tests.log\" 2>&1; then\n        print_status \"SUCCESS\" \"Load testing completed\"\n        \n        # Extract key metrics\n        if command -v jq > /dev/null && [ -f \"$RESULTS_DIR/load_test_$TIMESTAMP.json\" ]; then\n            AVG_RESPONSE=$(jq -r '.aggregate.latency.mean' \"$RESULTS_DIR/load_test_$TIMESTAMP.json\" 2>/dev/null || echo \"unknown\")\n            RPS=$(jq -r '.aggregate.rps.mean' \"$RESULTS_DIR/load_test_$TIMESTAMP.json\" 2>/dev/null || echo \"unknown\")\n            print_status \"INFO\" \"Average response time: ${AVG_RESPONSE}ms, RPS: $RPS\"\n        fi\n        LOAD_STATUS=\"success\"\n    else\n        print_status \"ERROR\" \"Load testing failed\"\n        LOAD_STATUS=\"failed\"\n    fi\n    \n    end_time=$(date +%s)\n    log_result \"Load Testing\" \"$LOAD_STATUS\" $((end_time - start_time)) \"Artillery.js performance testing\"\nelse\n    print_status \"WARNING\" \"Skipping load tests - server not running\"\n    LOAD_STATUS=\"skipped\"\nfi\n\n# Phase 6: Accessibility Testing\nif [ \"$SERVER_RUNNING\" = true ]; then\n    print_status \"INFO\" \"â™¿ Phase 6: Accessibility Testing\"\n    start_time=$(date +%s)\n    \n    if node tests/accessibility/accessibility-audit.js > \"$LOGS_DIR/accessibility.log\" 2>&1; then\n        print_status \"SUCCESS\" \"Accessibility audit completed\"\n        \n        # Count issues if available\n        if grep -q \"issues found\" \"$LOGS_DIR/accessibility.log\"; then\n            ISSUES=$(grep \"issues found\" \"$LOGS_DIR/accessibility.log\" | head -1 | awk '{print $1}')\n            print_status \"INFO\" \"Accessibility issues: $ISSUES\"\n        fi\n        A11Y_STATUS=\"success\"\n    else\n        print_status \"ERROR\" \"Accessibility testing failed\"\n        A11Y_STATUS=\"failed\"\n    fi\n    \n    end_time=$(date +%s)\n    log_result \"Accessibility\" \"$A11Y_STATUS\" $((end_time - start_time)) \"Pa11y WCAG 2.1 AA validation\"\nelse\n    print_status \"WARNING\" \"Skipping accessibility tests - server not running\"\n    A11Y_STATUS=\"skipped\"\nfi\n\n# Phase 7: Compliance Testing\nprint_status \"INFO\" \"ðŸ“‹ Phase 7: Compliance Testing\"\nstart_time=$(date +%s)\n\n# GDPR Compliance\nprint_status \"INFO\" \"Running GDPR compliance checks...\"\nif node tests/compliance/gdpr-compliance.js > \"$LOGS_DIR/gdpr_compliance.log\" 2>&1; then\n    print_status \"SUCCESS\" \"GDPR compliance checks passed\"\n    GDPR_STATUS=\"success\"\nelse\n    print_status \"ERROR\" \"GDPR compliance checks failed\"\n    GDPR_STATUS=\"failed\"\nfi\n\n# SOC2 Compliance\nprint_status \"INFO\" \"Running SOC2 compliance checks...\"\nif node tests/compliance/soc2-compliance.js > \"$LOGS_DIR/soc2_compliance.log\" 2>&1; then\n    print_status \"SUCCESS\" \"SOC2 compliance checks passed\"\n    SOC2_STATUS=\"success\"\nelse\n    print_status \"ERROR\" \"SOC2 compliance checks failed\"\n    SOC2_STATUS=\"failed\"\nfi\n\nend_time=$(date +%s)\nlog_result \"Compliance Testing\" \"$([ $GDPR_STATUS = success ] && [ $SOC2_STATUS = success ] && echo success || echo failed)\" $((end_time - start_time)) \"GDPR: $GDPR_STATUS, SOC2: $SOC2_STATUS\"\n\n# Phase 8: A/B Testing Framework\nprint_status \"INFO\" \"ðŸ§ª Phase 8: A/B Testing Framework\"\nstart_time=$(date +%s)\n\n# Validate A/B test configurations\nif npm run ab-test:validate > \"$LOGS_DIR/ab_testing.log\" 2>&1; then\n    print_status \"SUCCESS\" \"A/B test configurations validated\"\n    \n    # List active tests\n    TEST_COUNT=$(npm run ab-test:list 2>/dev/null | grep -c \"âœ…\" || echo 0)\n    print_status \"INFO\" \"Active A/B tests: $TEST_COUNT\"\n    AB_STATUS=\"success\"\nelse\n    print_status \"ERROR\" \"A/B test validation failed\"\n    AB_STATUS=\"failed\"\nfi\n\nend_time=$(date +%s)\nlog_result \"A/B Testing\" \"$AB_STATUS\" $((end_time - start_time)) \"PostHog integration and test validation\"\n\n# Phase 9: End-to-End Testing\nif [ \"$SERVER_RUNNING\" = true ]; then\n    print_status \"INFO\" \"ðŸ”„ Phase 9: End-to-End Testing\"\n    start_time=$(date +%s)\n    \n    if npm run test:e2e > \"$LOGS_DIR/e2e_tests.log\" 2>&1; then\n        print_status \"SUCCESS\" \"End-to-end tests passed\"\n        E2E_STATUS=\"success\"\n    else\n        print_status \"ERROR\" \"End-to-end tests failed\"\n        E2E_STATUS=\"failed\"\n    fi\n    \n    end_time=$(date +%s)\n    log_result \"End-to-End Testing\" \"$E2E_STATUS\" $((end_time - start_time)) \"Playwright browser automation\"\nelse\n    print_status \"WARNING\" \"Skipping E2E tests - server not running\"\n    E2E_STATUS=\"skipped\"\nfi\n\n# Cleanup: Stop development server if we started it\nif [ -n \"${DEV_SERVER_PID:-}\" ]; then\n    print_status \"INFO\" \"Stopping development server...\"\n    kill $DEV_SERVER_PID 2>/dev/null || true\nfi\n\n# Close test report JSON\nsed -i '' '$ s/,$//' \"$RESULTS_DIR/$TEST_REPORT\" 2>/dev/null || sed -i '$ s/,$//' \"$RESULTS_DIR/$TEST_REPORT\"\necho \"]\" >> \"$RESULTS_DIR/$TEST_REPORT\"\n\n# Final Summary\nprint_status \"INFO\" \"ðŸ“Š Testing Summary\"\nprint_status \"INFO\" \"=================\"\n\n# Count results\nTOTAL_TESTS=0\nPASSED_TESTS=0\nFAILED_TESTS=0\nSKIPPED_TESTS=0\n\nfor status in \"$TS_STATUS\" \"$UNIT_STATUS\" \"$VISUAL_STATUS\" \"$LOAD_STATUS\" \"$A11Y_STATUS\" \"$GDPR_STATUS\" \"$SOC2_STATUS\" \"$AB_STATUS\" \"$E2E_STATUS\"; do\n    TOTAL_TESTS=$((TOTAL_TESTS + 1))\n    case $status in\n        \"success\") PASSED_TESTS=$((PASSED_TESTS + 1)) ;;\n        \"failed\") FAILED_TESTS=$((FAILED_TESTS + 1)) ;;\n        \"skipped\") SKIPPED_TESTS=$((SKIPPED_TESTS + 1)) ;;\n    esac\ndone\n\nprint_status \"INFO\" \"Total Tests: $TOTAL_TESTS\"\nprint_status \"SUCCESS\" \"Passed: $PASSED_TESTS\"\nif [ $FAILED_TESTS -gt 0 ]; then\n    print_status \"ERROR\" \"Failed: $FAILED_TESTS\"\nfi\nif [ $SKIPPED_TESTS -gt 0 ]; then\n    print_status \"WARNING\" \"Skipped: $SKIPPED_TESTS\"\nfi\n\nSUCCESS_RATE=$((PASSED_TESTS * 100 / (TOTAL_TESTS - SKIPPED_TESTS)))\nprint_status \"INFO\" \"Success Rate: $SUCCESS_RATE%\"\n\n# Generate summary report\nSUMMARY_FILE=\"$RESULTS_DIR/qa_summary_$TIMESTAMP.txt\"\ncat > \"$SUMMARY_FILE\" << EOF\nFormaOS Quality Assurance Testing Report\n========================================\nTimestamp: $(date)\nDuration: $(($(date +%s) - $(cat \"$RESULTS_DIR/$TEST_REPORT\" | head -1 | grep -o '[0-9]\\{10\\}' | head -1 || echo 0))) seconds\n\nTest Results:\n- Environment Setup: $([ \"$SERVER_RUNNING\" = true ] && echo \"âœ… PASSED\" || echo \"âŒ FAILED\")\n- TypeScript Check: $([ \"$TS_STATUS\" = \"success\" ] && echo \"âœ… PASSED\" || echo \"âŒ FAILED\")\n- Unit Testing: $([ \"$UNIT_STATUS\" = \"success\" ] && echo \"âœ… PASSED\" || echo \"âŒ FAILED\")\n- Visual Regression: $([ \"$VISUAL_STATUS\" = \"success\" ] && echo \"âœ… PASSED\" || [ \"$VISUAL_STATUS\" = \"skipped\" ] && echo \"â­ï¸ SKIPPED\" || echo \"âŒ FAILED\")\n- Load Testing: $([ \"$LOAD_STATUS\" = \"success\" ] && echo \"âœ… PASSED\" || [ \"$LOAD_STATUS\" = \"skipped\" ] && echo \"â­ï¸ SKIPPED\" || echo \"âŒ FAILED\")\n- Accessibility: $([ \"$A11Y_STATUS\" = \"success\" ] && echo \"âœ… PASSED\" || [ \"$A11Y_STATUS\" = \"skipped\" ] && echo \"â­ï¸ SKIPPED\" || echo \"âŒ FAILED\")\n- GDPR Compliance: $([ \"$GDPR_STATUS\" = \"success\" ] && echo \"âœ… PASSED\" || echo \"âŒ FAILED\")\n- SOC2 Compliance: $([ \"$SOC2_STATUS\" = \"success\" ] && echo \"âœ… PASSED\" || echo \"âŒ FAILED\")\n- A/B Testing: $([ \"$AB_STATUS\" = \"success\" ] && echo \"âœ… PASSED\" || echo \"âŒ FAILED\")\n- End-to-End: $([ \"$E2E_STATUS\" = \"success\" ] && echo \"âœ… PASSED\" || [ \"$E2E_STATUS\" = \"skipped\" ] && echo \"â­ï¸ SKIPPED\" || echo \"âŒ FAILED\")\n\nOverall Success Rate: $SUCCESS_RATE%\n\nLogs Location: $LOGS_DIR/\nResults Location: $RESULTS_DIR/\nDetailed Report: $TEST_REPORT\nEOF\n\nprint_status \"INFO\" \"ðŸ“‹ Summary report saved to: $SUMMARY_FILE\"\nprint_status \"INFO\" \"ðŸ“ Detailed logs available in: $LOGS_DIR/\"\nprint_status \"INFO\" \"ðŸŽ¯ Test completed at: $(date)\"\n\n# Exit with appropriate code\nif [ $FAILED_TESTS -eq 0 ]; then\n    print_status \"SUCCESS\" \"ðŸŽ‰ All tests completed successfully!\"\n    exit 0\nelse\n    print_status \"ERROR\" \"âŒ Some tests failed. Check logs for details.\"\n    exit 1\nfi